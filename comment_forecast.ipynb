{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vk_api import VkApi\n",
    "import vk_api\n",
    "import vk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from nltk.tokenize.casual import TweetTokenizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≥–Ω–æ–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ—Å—Ç–æ–≤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id –≥—Ä—É–ø–ø –≤–∫\n",
    "\n",
    "id_avia = '-39283725'\n",
    "id_moscow = '-137324015'\n",
    "id_nazem = '-212086355'\n",
    "mas_id = [id_avia, id_moscow, id_nazem]\n",
    "token = 'vk1.a.J6_ahUxPyWhFaYbShB5hkJ4hAYpwRE8oanqh1vLBPZOQ8bo7RzICWE1yuEhUs1iMWYTI-zalDHRr8V6NAS-12S-qYImDQT7sVejSI3_3SH1uaAIWrxkC5tljaQDAwd7O17rCrccRDEvf09rreDLBfPsVhRCQYdK_Yelswjs1homcD1B6MFHTVqvrfmyV2gZ_30kGH19ZsdDn3IA0EWZszg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≥—Ä—É–ø–ø –≤–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39283725 0\n",
      "-39283725 100\n",
      "-39283725 200\n",
      "–ü–æ—Å—Ç–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏:  300\n",
      "–ü–æ—Å—Ç–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏:  0\n",
      "-137324015 100\n",
      "-137324015 200\n",
      "-137324015 300\n",
      "-212086355 0\n",
      "-212086355 100\n",
      "-212086355 200\n",
      "-212086355 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open('C:/Users/Admin/Desktop/token.txt') as f:\n",
    "#      token = f.readlines()[0]\n",
    "\n",
    "#token = 'https: // oauth.vk.com/blank.html # access_token=vk1.a.J6_ahUxPyWhFaYbShB5hkJ4hAYpwRE8oanqh1vLBPZOQ8bo7RzICWE1yuEhUs1iMWYTI-zalDHRr8V6NAS-12S-qYImDQT7sVejSI3_3SH1uaAIWrxkC5tljaQDAwd7O17rCrccRDEvf09rreDLBfPsVhRCQYdK_Yelswjs1homcD1B6MFHTVqvrfmyV2gZ_30kGH19ZsdDn3IA0EWZszg&expires_in=0&user_id=257606113'\n",
    "token = token\n",
    "\n",
    "\n",
    "def main(offset: int, token: str, group_id: str):\n",
    "    vk = vk_api.VkApi(token=token)  # –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ)\n",
    "    api = vk.get_api()\n",
    "    posts = api.wall.get(owner_id=group_id, offset=offset, count=100)['items']\n",
    "    posts_strings = [post['text'] for post in posts]\n",
    "    num_like = []\n",
    "    comments_strings = []\n",
    "    for post in posts:\n",
    "        comments = api.wall.getComments(\n",
    "            owner_id=group_id, post_id=post['id'], count=100)['items']\n",
    "        comments_strings.append([comment['text'] for comment in comments])\n",
    "        itemID = post['id']\n",
    "        isLiked = api.likes.getList(\n",
    "            type='post',\n",
    "            owner_id=group_id,\n",
    "            item_id=itemID\n",
    "        )\n",
    "        num_like.append(isLiked['count'])\n",
    "    return posts_strings, comments_strings, num_like\n",
    "\n",
    "\n",
    "combo_list_posts = []\n",
    "combolist_comments = []\n",
    "combolist_like = []\n",
    "\n",
    "for group_id in mas_id:\n",
    "    for i in range(0, 301, 100):\n",
    "        try:\n",
    "            rzd_posts, comments_strings_rzd, rzd_like_count = main(\n",
    "                offset=i, token=token, group_id=group_id)\n",
    "            combo_list_posts.extend(rzd_posts)\n",
    "            combolist_comments.extend(comments_strings_rzd)\n",
    "            combolist_like.extend(rzd_like_count)\n",
    "            print(group_id, i)\n",
    "        except:\n",
    "            print('–ü–æ—Å—Ç–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–º–µ—â–µ–Ω–∏–∏: ', i)\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combolist_comments_count = [len(comments) for comments in combolist_comments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–ü–æ—Å—Ç—ã</th>\n",
       "      <th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ü—É–ª–∫–æ–≤–æ (LED) —Å—Ç–∞–ª –¥–≤–∞–¥—Ü–∞—Ç—ã–º –∞—ç—Ä...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í—ã–±—Ä–∞–≤—à–∞—è –ø—É—Ç—å –ø–æ–ª–Ω–æ–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Å...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ö–æ–º–ø–∞–Ω–∏—è \"–°–∏—Ä–µ–Ω–∞\", –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å—Ç–∞–≤—â–∏...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢—É—Ä–∫–º–µ–Ω—Å–∫–∞—è –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è Turkmenistan Airlines...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ö—ç–º–ø–±–µ–ª–ª –£–∏–ª—Å–æ–Ω, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç Air India, –≤ –æ—á–µ—Ä–µ–¥...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>üñ•–í –ø–∞—Ä–∫–∞—Ö –ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å–∞ –Ω–∞—á–∞–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ü–ê–ö...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>üöê–ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–≥–æ –∫–ª...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>üéü–í —Ç—Ä–∞–º–≤–∞—è—Ö –ê –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä—ã –ø–æ–¥–∞—Ä–∏–ª–∏ ¬´–ï–¥–∏–Ω—ã–µ¬ª ‚Äî —Ç...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 –ü–æ—Å—Ç—ã  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\n",
       "0    –ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ü—É–ª–∫–æ–≤–æ (LED) —Å—Ç–∞–ª –¥–≤–∞–¥—Ü–∞—Ç—ã–º –∞—ç—Ä...            1\n",
       "1    –í—ã–±—Ä–∞–≤—à–∞—è –ø—É—Ç—å –ø–æ–ª–Ω–æ–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Å...            1\n",
       "2    –ö–æ–º–ø–∞–Ω–∏—è \"–°–∏—Ä–µ–Ω–∞\", –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å—Ç–∞–≤—â–∏...            1\n",
       "3    –¢—É—Ä–∫–º–µ–Ω—Å–∫–∞—è –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è Turkmenistan Airlines...            1\n",
       "4    –ö—ç–º–ø–±–µ–ª–ª –£–∏–ª—Å–æ–Ω, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç Air India, –≤ –æ—á–µ—Ä–µ–¥...            1\n",
       "..                                                 ...          ...\n",
       "995                                                               1\n",
       "996  üñ•–í –ø–∞—Ä–∫–∞—Ö –ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å–∞ –Ω–∞—á–∞–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ü–ê–ö...            1\n",
       "997  üöê–ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–≥–æ –∫–ª...           19\n",
       "998                                                               4\n",
       "999  üéü–í —Ç—Ä–∞–º–≤–∞—è—Ö –ê –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä—ã –ø–æ–¥–∞—Ä–∏–ª–∏ ¬´–ï–¥–∏–Ω—ã–µ¬ª ‚Äî —Ç...            1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'–ü–æ—Å—Ç—ã': combo_list_posts,\n",
    "                  '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏': combolist_comments_count})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "–ü–æ—Å—Ç—ã          0\n",
       "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.034979423868313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6260\\2142041351.py:1: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df = df.astype({'–ü–æ—Å—Ç—ã': np.str})\n"
     ]
    }
   ],
   "source": [
    "df = df.astype({'–ü–æ—Å—Ç—ã': np.str})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "    clean_text = ' '.join(tokens)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "df['clean_text'] = df['–ü–æ—Å—Ç—ã'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>–ü–æ—Å—Ç—ã</th>\n",
       "      <th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ü—É–ª–∫–æ–≤–æ (LED) —Å—Ç–∞–ª –¥–≤–∞–¥—Ü–∞—Ç—ã–º –∞—ç—Ä...</td>\n",
       "      <td>1</td>\n",
       "      <td>–ø–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ø—É–ª–∫–æ–≤–æ ( led ) —Å—Ç–∞—Ç—å –¥–≤–∞–¥—Ü–∞—Ç—ã–π ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í—ã–±—Ä–∞–≤—à–∞—è –ø—É—Ç—å –ø–æ–ª–Ω–æ–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Å...</td>\n",
       "      <td>1</td>\n",
       "      <td>–≤—ã–±—Ä–∞—Ç—å –ø—É—Ç—å –ø–æ–ª–Ω—ã–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ö–æ–º–ø–∞–Ω–∏—è \"–°–∏—Ä–µ–Ω–∞\", –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å—Ç–∞–≤—â–∏...</td>\n",
       "      <td>1</td>\n",
       "      <td>–∫–æ–º–ø–∞–Ω–∏—è `` —Å–∏—Ä–µ–Ω–∞ '' , –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–¢—É—Ä–∫–º–µ–Ω—Å–∫–∞—è –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è Turkmenistan Airlines...</td>\n",
       "      <td>1</td>\n",
       "      <td>—Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è turkmenistan airline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ö—ç–º–ø–±–µ–ª–ª –£–∏–ª—Å–æ–Ω, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç Air India, –≤ –æ—á–µ—Ä–µ–¥...</td>\n",
       "      <td>1</td>\n",
       "      <td>–∫—ç–º–ø–±–µ–ª–ª–∞ —É–∏–ª—Å–æ–Ω , –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç air india , –æ—á–µ—Ä–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>üöã–û–¥–∏–Ω ¬´–í–∏—Ç—è–∑—å¬ª –≤–µ—Å–∏—Ç –∫–∞–∫ 40 –ª–µ–≥–∫–æ–≤—É—à–µ–∫ –∏–ª–∏ —Å—Ç–∞...</td>\n",
       "      <td>7</td>\n",
       "      <td>üöã–æ–¥–∏–Ω ¬´ –≤–∏—Ç—è–∑—å ¬ª –≤–µ—Å–∏—Ç—å 40 –ª–µ–≥–∫–æ–≤—É—à–∫–∞ —Å—Ç–∞–¥–æ –ø–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>üê±–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ ¬´–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–µ—Ä–µ–≤–æ–∑–æ–∫¬ª –≤–∑—è–ª–∏ –ø–æ–¥...</td>\n",
       "      <td>1</td>\n",
       "      <td>üê±—Å–æ—Ç—Ä—É–¥–Ω–∏–∫ ¬´ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–µ–≤–æ–∑–∫–∞ ¬ª –≤–∑—è—Ç—å –æ–ø–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>üñ•–í –ø–∞—Ä–∫–∞—Ö –ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å–∞ –Ω–∞—á–∞–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ü–ê–ö...</td>\n",
       "      <td>1</td>\n",
       "      <td>üñ•–≤ –ø–∞—Ä–∫–∞ –º–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –Ω–∞—á–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–∞–∫ ‚Äî...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>üöê–ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–≥–æ –∫–ª...</td>\n",
       "      <td>19</td>\n",
       "      <td>üöê–º–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–µ –∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>üéü–í —Ç—Ä–∞–º–≤–∞—è—Ö –ê –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä—ã –ø–æ–¥–∞—Ä–∏–ª–∏ ¬´–ï–¥–∏–Ω—ã–µ¬ª ‚Äî —Ç...</td>\n",
       "      <td>1</td>\n",
       "      <td>üéü–≤ —Ç—Ä–∞–º–≤–∞–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –ø–æ–¥–∞—Ä–∏—Ç—å ¬´ –µ–¥–∏–Ω—ã–π ¬ª ‚Äî —Ö–æ—Ç...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>972 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 –ü–æ—Å—Ç—ã  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏  \\\n",
       "0    –ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ü—É–ª–∫–æ–≤–æ (LED) —Å—Ç–∞–ª –¥–≤–∞–¥—Ü–∞—Ç—ã–º –∞—ç—Ä...            1   \n",
       "1    –í—ã–±—Ä–∞–≤—à–∞—è –ø—É—Ç—å –ø–æ–ª–Ω–æ–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —Å...            1   \n",
       "2    –ö–æ–º–ø–∞–Ω–∏—è \"–°–∏—Ä–µ–Ω–∞\", –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å—Ç–∞–≤—â–∏...            1   \n",
       "3    –¢—É—Ä–∫–º–µ–Ω—Å–∫–∞—è –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è Turkmenistan Airlines...            1   \n",
       "4    –ö—ç–º–ø–±–µ–ª–ª –£–∏–ª—Å–æ–Ω, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç Air India, –≤ –æ—á–µ—Ä–µ–¥...            1   \n",
       "..                                                 ...          ...   \n",
       "993  üöã–û–¥–∏–Ω ¬´–í–∏—Ç—è–∑—å¬ª –≤–µ—Å–∏—Ç –∫–∞–∫ 40 –ª–µ–≥–∫–æ–≤—É—à–µ–∫ –∏–ª–∏ —Å—Ç–∞...            7   \n",
       "994  üê±–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ ¬´–û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø–µ—Ä–µ–≤–æ–∑–æ–∫¬ª –≤–∑—è–ª–∏ –ø–æ–¥...            1   \n",
       "996  üñ•–í –ø–∞—Ä–∫–∞—Ö –ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å–∞ –Ω–∞—á–∞–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ü–ê–ö...            1   \n",
       "997  üöê–ú–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–≥–æ –∫–ª...           19   \n",
       "999  üéü–í —Ç—Ä–∞–º–≤–∞—è—Ö –ê –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä—ã –ø–æ–¥–∞—Ä–∏–ª–∏ ¬´–ï–¥–∏–Ω—ã–µ¬ª ‚Äî —Ç...            1   \n",
       "\n",
       "                                            clean_text  \n",
       "0    –ø–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –ø—É–ª–∫–æ–≤–æ ( led ) —Å—Ç–∞—Ç—å –¥–≤–∞–¥—Ü–∞—Ç—ã–π ...  \n",
       "1    –≤—ã–±—Ä–∞—Ç—å –ø—É—Ç—å –ø–æ–ª–Ω—ã–π –∏–º–ø–æ—Ä—Ç–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç...  \n",
       "2    –∫–æ–º–ø–∞–Ω–∏—è `` —Å–∏—Ä–µ–Ω–∞ '' , –≤–µ–¥—É—â–∏–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –ø–æ—Å...  \n",
       "3    —Ç—É—Ä–∫–º–µ–Ω—Å–∫–∏–π –∞–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è turkmenistan airline ...  \n",
       "4    –∫—ç–º–ø–±–µ–ª–ª–∞ —É–∏–ª—Å–æ–Ω , –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç air india , –æ—á–µ—Ä–µ...  \n",
       "..                                                 ...  \n",
       "993  üöã–æ–¥–∏–Ω ¬´ –≤–∏—Ç—è–∑—å ¬ª –≤–µ—Å–∏—Ç—å 40 –ª–µ–≥–∫–æ–≤—É—à–∫–∞ —Å—Ç–∞–¥–æ –ø–æ...  \n",
       "994  üê±—Å–æ—Ç—Ä—É–¥–Ω–∏–∫ ¬´ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–µ–≤–æ–∑–∫–∞ ¬ª –≤–∑—è—Ç—å –æ–ø–µ...  \n",
       "996  üñ•–≤ –ø–∞—Ä–∫–∞ –º–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –Ω–∞—á–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–∞–∫ ‚Äî...  \n",
       "997  üöê–º–æ—Å–≥–æ—Ä—Ç—Ä–∞–Ω—Å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —ç–ª–µ–∫—Ç—Ä–æ–±—É—Å –º–∞–ª–æ–µ –∫...  \n",
       "999  üéü–≤ —Ç—Ä–∞–º–≤–∞–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –ø–æ–¥–∞—Ä–∏—Ç—å ¬´ –µ–¥–∏–Ω—ã–π ¬ª ‚Äî —Ö–æ—Ç...  \n",
       "\n",
       "[972 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 2s 22ms/step - loss: 65.7057 - mae: 4.8839\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 60.6451 - mae: 4.5000\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 51.6212 - mae: 3.9422\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 41.9745 - mae: 3.5829\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 35.1649 - mae: 3.4509\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 30.5630 - mae: 3.0758\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 26.1853 - mae: 2.5253\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 22.3461 - mae: 2.2298\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 18.9460 - mae: 1.8542\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 16.0621 - mae: 1.5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197010d6bf0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_data = df['clean_text']\n",
    "labels = df['–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏']\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train).toarray() \n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 50.0647 - mae: 3.1415\n",
      "Test Loss: 50.064735412597656, Test MAE: 3.1414921283721924\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test MAE: {mae}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
